{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Project overview dev-lab is an efficient, fast, and easily configurable dev environment: a self-hosting platform for any web services. The dev-lab is desingned to host any helper services for development work or personal usage. It is also a great sandbox for learning new technologies. Hardware The current set of devices used for the project includes: 1x Asus Tinker Board (Raspberry Pi analog): CPU: 4x core Rockchip RK3288 ARM Cortex-A17 1.8GHz RAM: 2GB microSD: 32GB Features: 1G Ethernet , WiFi , Bluetooth , 40 pins GPIO , 4x USB 2.0 OS: Debian 9 2x Lenovo ThinkCentre M600 Tiny : CPU: 4x core Intel Pentium J3710 @ 1.60GHz RAM: 8GB SSD: 128GB Features: 1G Ethernet , 6x USB 3.0 , WOL , PXE OS: Ubuntu 20.04 Architecture In the context of hardware, the platform can be divided into 2 parts: the control plane and the main cluster. The Control Plane (hosted on ASUS Tinker Board) has services to monitor and manage all the services of the whole platform. Hierarchically, the architecture looks like a stack, which is based on bare metal, and on top of the stack is the application layer, that is, services that are ready for use. The whole infrastructure should be provisioned automatically starting from bottom to top: Hardware layer - bare metal. The setup on this level is manual: Enable WOL. Wake On Lan - protocol for waking computers up from a very low power mode remotely Enable PXE. Preboot execution environment - for OS installation over network OS layer : OS installation via PXE System configuration using Ansible Infrastructure layer : Install docker and docker-compose Install k3s - lightweight Kubernetes cluster Service layer : Install portainer agents on every device and portainer server on control plane Install local DNS server on control plane Install homeassistant on control plane and load configuration. HA is using for power on/off devices using web UI and monitoring system resources (RAM, CPU, storage) for every device Install monitoring tools: Glances on cluster devices Uptime Kuma on control plane, configure Uptime Kuma Install Gitea git server Install Drone CI/CD system Application layer : Deploy custom services (like kafkasender) on Kubernetes Deploy services required for development work on Docker using Portainer Features Power on/off devices via UI Fast docker stacks management via UI Private DNS server DNS sinkhole used as network-wide ad blocker Dashboard UI to access hosted services Uptime monitoring system to monitor which services and devices are up or down, alerting to messengers (Telegram bot) Git server backed up with Github CI/CD platform Documentation hosted on Kubernetes, backed up with Github pages Automated infrastructure installation and management (k3s, docker) Automatic OS provisioning Whole infrastructure provisioning with single command Managing Kubernetes using GitOps Remote IDE (Intellij IDEA server) Task management system Configuration management system for local development Everything is defined as code, use GitOps for everything Secrets and encryption management Centralized logs management system HTTPS everywhere Single sign-on Automatic backups Private container registry Expose some services to the internet securely (VPN server) Automatic OS updates Screenshots Some screenshots are shown here (click to enlarge). They can't capture all the project's features, but they are sufficient to get a concept of it. Dashboard powered by Homer Uptime Kuma - uptime monitor Devices management dashboard powered by Home Assistant PiHole - DNS server/Ad blocker Portainer - Docker management Gitea - Git server Drone - CI/CD platform Longhorn - Persistent storage for Kubernetes","title":"Project overview"},{"location":"#project-overview","text":"dev-lab is an efficient, fast, and easily configurable dev environment: a self-hosting platform for any web services. The dev-lab is desingned to host any helper services for development work or personal usage. It is also a great sandbox for learning new technologies.","title":"Project overview"},{"location":"#hardware","text":"The current set of devices used for the project includes: 1x Asus Tinker Board (Raspberry Pi analog): CPU: 4x core Rockchip RK3288 ARM Cortex-A17 1.8GHz RAM: 2GB microSD: 32GB Features: 1G Ethernet , WiFi , Bluetooth , 40 pins GPIO , 4x USB 2.0 OS: Debian 9 2x Lenovo ThinkCentre M600 Tiny : CPU: 4x core Intel Pentium J3710 @ 1.60GHz RAM: 8GB SSD: 128GB Features: 1G Ethernet , 6x USB 3.0 , WOL , PXE OS: Ubuntu 20.04","title":"Hardware"},{"location":"#architecture","text":"In the context of hardware, the platform can be divided into 2 parts: the control plane and the main cluster. The Control Plane (hosted on ASUS Tinker Board) has services to monitor and manage all the services of the whole platform. Hierarchically, the architecture looks like a stack, which is based on bare metal, and on top of the stack is the application layer, that is, services that are ready for use. The whole infrastructure should be provisioned automatically starting from bottom to top: Hardware layer - bare metal. The setup on this level is manual: Enable WOL. Wake On Lan - protocol for waking computers up from a very low power mode remotely Enable PXE. Preboot execution environment - for OS installation over network OS layer : OS installation via PXE System configuration using Ansible Infrastructure layer : Install docker and docker-compose Install k3s - lightweight Kubernetes cluster Service layer : Install portainer agents on every device and portainer server on control plane Install local DNS server on control plane Install homeassistant on control plane and load configuration. HA is using for power on/off devices using web UI and monitoring system resources (RAM, CPU, storage) for every device Install monitoring tools: Glances on cluster devices Uptime Kuma on control plane, configure Uptime Kuma Install Gitea git server Install Drone CI/CD system Application layer : Deploy custom services (like kafkasender) on Kubernetes Deploy services required for development work on Docker using Portainer","title":"Architecture"},{"location":"#features","text":"Power on/off devices via UI Fast docker stacks management via UI Private DNS server DNS sinkhole used as network-wide ad blocker Dashboard UI to access hosted services Uptime monitoring system to monitor which services and devices are up or down, alerting to messengers (Telegram bot) Git server backed up with Github CI/CD platform Documentation hosted on Kubernetes, backed up with Github pages Automated infrastructure installation and management (k3s, docker) Automatic OS provisioning Whole infrastructure provisioning with single command Managing Kubernetes using GitOps Remote IDE (Intellij IDEA server) Task management system Configuration management system for local development Everything is defined as code, use GitOps for everything Secrets and encryption management Centralized logs management system HTTPS everywhere Single sign-on Automatic backups Private container registry Expose some services to the internet securely (VPN server) Automatic OS updates","title":"Features"},{"location":"#screenshots","text":"Some screenshots are shown here (click to enlarge). They can't capture all the project's features, but they are sufficient to get a concept of it. Dashboard powered by Homer Uptime Kuma - uptime monitor Devices management dashboard powered by Home Assistant PiHole - DNS server/Ad blocker Portainer - Docker management Gitea - Git server Drone - CI/CD platform Longhorn - Persistent storage for Kubernetes","title":"Screenshots"},{"location":"guides/drone/","text":"Drone installation ok k3s Drone is a self-service Continuous Integration platform for busy development teams. Project page: https://www.drone.io Requirements k3s cluster kubectl helm git server installed on cluster Installation Create a namespace for drone server and runners kubectl create namespace drone Install drone server Gitea server should be already installed. Go to Settings -> Applications -> Create application Copy Client ID and Client Secret to server-values.yaml and add gitea server url: DRONE_GITEA_CLIENT_ID DRONE_GITEA_CLIENT_SECRET DRONE_GITEA_SERVER Add Helm repo: helm repo add drone https://charts.drone.io helm repo update Install drone server: helm install drone drone/drone -f server-values.yaml --namespace drone Install drone runner helm install drone-runner-kube drone/drone-runner-kube -f runner-values.yaml --namespace drone More details here Expose Drone installation The DNS record pointing to cluster IP should be created, the host should be set to ingress.yaml (spec.rules.host) Expose http UI using Ingress service: kubectl apply -f ingress.yaml","title":"Drone installation ok k3s"},{"location":"guides/drone/#drone-installation-ok-k3s","text":"Drone is a self-service Continuous Integration platform for busy development teams. Project page: https://www.drone.io","title":"Drone installation ok k3s"},{"location":"guides/drone/#requirements","text":"k3s cluster kubectl helm git server installed on cluster","title":"Requirements"},{"location":"guides/drone/#installation","text":"","title":"Installation"},{"location":"guides/drone/#create-a-namespace-for-drone-server-and-runners","text":"kubectl create namespace drone","title":"Create a namespace for drone server and runners"},{"location":"guides/drone/#install-drone-server","text":"Gitea server should be already installed. Go to Settings -> Applications -> Create application Copy Client ID and Client Secret to server-values.yaml and add gitea server url: DRONE_GITEA_CLIENT_ID DRONE_GITEA_CLIENT_SECRET DRONE_GITEA_SERVER Add Helm repo: helm repo add drone https://charts.drone.io helm repo update Install drone server: helm install drone drone/drone -f server-values.yaml --namespace drone","title":"Install drone server"},{"location":"guides/drone/#install-drone-runner","text":"helm install drone-runner-kube drone/drone-runner-kube -f runner-values.yaml --namespace drone More details here","title":"Install drone runner"},{"location":"guides/drone/#expose-drone-installation","text":"The DNS record pointing to cluster IP should be created, the host should be set to ingress.yaml (spec.rules.host) Expose http UI using Ingress service: kubectl apply -f ingress.yaml","title":"Expose Drone installation"},{"location":"guides/gitea/","text":"Gitea installation on k3s Gitea is a painless self-hosted Git service. It is similar to GitHub, Bitbucket, and GitLab. Project page: https://gitea.io Requirements k3s cluster kubectl helm Longhorn installed on cluster Installation Create a namespace for gitea and related stuff kubectl create namespace git Install postgres kubectl apply -f postgres The password for the postgres user is specified by the POSTGRESS_PASSWORD environment variable, which uses a secret ( postgres/secret.yaml ) The data volume uses a PersistentVolumeClaim backed by Longhorn, so that it doesn't get deleted when the pod is restarted ( postgres/statefulset.yaml ) Database is created by using initialization scripts in a ConfigMap. ( postgres/configmap.yaml ) The postgres instance needs to be exposed to the Gitea instance; use a ClusterIP service for that ( postgres/service.yaml ) Install Gitea using Helm Gitea Helm Chart helm repo add gitea-charts https://dl.gitea.io/charts/ helm repo update helm install gitea gitea-charts/gitea --namespace git --values values.yaml Expose Gitea installation The DNS record pointing to cluster IP should be created, the host should be set to ingress.yaml (spec.rules.host) Expose http UI using Ingress service: kubectl apply -f ingress.yaml","title":"Gitea installation on k3s"},{"location":"guides/gitea/#gitea-installation-on-k3s","text":"Gitea is a painless self-hosted Git service. It is similar to GitHub, Bitbucket, and GitLab. Project page: https://gitea.io","title":"Gitea installation on k3s"},{"location":"guides/gitea/#requirements","text":"k3s cluster kubectl helm Longhorn installed on cluster","title":"Requirements"},{"location":"guides/gitea/#installation","text":"","title":"Installation"},{"location":"guides/gitea/#create-a-namespace-for-gitea-and-related-stuff","text":"kubectl create namespace git","title":"Create a namespace for gitea and related stuff"},{"location":"guides/gitea/#install-postgres","text":"kubectl apply -f postgres The password for the postgres user is specified by the POSTGRESS_PASSWORD environment variable, which uses a secret ( postgres/secret.yaml ) The data volume uses a PersistentVolumeClaim backed by Longhorn, so that it doesn't get deleted when the pod is restarted ( postgres/statefulset.yaml ) Database is created by using initialization scripts in a ConfigMap. ( postgres/configmap.yaml ) The postgres instance needs to be exposed to the Gitea instance; use a ClusterIP service for that ( postgres/service.yaml )","title":"Install postgres"},{"location":"guides/gitea/#install-gitea-using-helm","text":"Gitea Helm Chart helm repo add gitea-charts https://dl.gitea.io/charts/ helm repo update helm install gitea gitea-charts/gitea --namespace git --values values.yaml","title":"Install Gitea using Helm"},{"location":"guides/gitea/#expose-gitea-installation","text":"The DNS record pointing to cluster IP should be created, the host should be set to ingress.yaml (spec.rules.host) Expose http UI using Ingress service: kubectl apply -f ingress.yaml","title":"Expose Gitea installation"},{"location":"guides/longhorn/","text":"Longhorn installation on k3s Longhorn is a highly available persistent storage for Kubernetes Project page: https://longhorn.io Requirements k3s cluster kubectl helm Longhorn installed on cluster Installation helm repo add longhorn https://charts.longhorn.io helm repo update helm install longhorn longhorn/longhorn --namespace longhorn --create-namespace Make Longhorn a default storageclass: kubectl patch storageclass local-path -p '{\"metadata\": {\"annotations\":{\"storageclass.kubernetes.io/is-default-class\":\"false\"}}}' Restarting a node causes the local-path storage class to be re-registered as the default. See this issue . Consider disabling it entirely, or just be aware that occasionally you\u2019ll get Internal error occurred: 2 default StorageClasses were found errors , and you\u2019ll have to patch it again. Create ingress for Longhorn UI: kubectl apply -f ingress.yaml --namespace longhorn","title":"Longhorn installation on k3s"},{"location":"guides/longhorn/#longhorn-installation-on-k3s","text":"Longhorn is a highly available persistent storage for Kubernetes Project page: https://longhorn.io","title":"Longhorn installation on k3s"},{"location":"guides/longhorn/#requirements","text":"k3s cluster kubectl helm Longhorn installed on cluster","title":"Requirements"},{"location":"guides/longhorn/#installation","text":"helm repo add longhorn https://charts.longhorn.io helm repo update helm install longhorn longhorn/longhorn --namespace longhorn --create-namespace Make Longhorn a default storageclass: kubectl patch storageclass local-path -p '{\"metadata\": {\"annotations\":{\"storageclass.kubernetes.io/is-default-class\":\"false\"}}}' Restarting a node causes the local-path storage class to be re-registered as the default. See this issue . Consider disabling it entirely, or just be aware that occasionally you\u2019ll get Internal error occurred: 2 default StorageClasses were found errors , and you\u2019ll have to patch it again. Create ingress for Longhorn UI: kubectl apply -f ingress.yaml --namespace longhorn","title":"Installation"},{"location":"guides/mkdocs/","text":"Build a documentation web site using mkdocs TBD ???","title":"Build a documentation web site using mkdocs"},{"location":"guides/mkdocs/#build-a-documentation-web-site-using-mkdocs","text":"TBD ???","title":"Build a documentation web site using mkdocs"},{"location":"guides/nginx-configure/","text":"Configuring nginx reverse proxy Nginx Proxy Manager is a NGINX server with easy to use UI. It listens on port 80 and 443 and routes the requests for your services (ip:port). First, you need to add a DNS record with domain, pointing to Nginx Proxy IP address. Then, add a new new proxy, click Add Proxy Host: After you save it, your service is accessible on your selected domain name.","title":"Configuring nginx reverse proxy"},{"location":"guides/nginx-configure/#configuring-nginx-reverse-proxy","text":"Nginx Proxy Manager is a NGINX server with easy to use UI. It listens on port 80 and 443 and routes the requests for your services (ip:port). First, you need to add a DNS record with domain, pointing to Nginx Proxy IP address. Then, add a new new proxy, click Add Proxy Host: After you save it, your service is accessible on your selected domain name.","title":"Configuring nginx reverse proxy"},{"location":"guides/pihole-dns/","text":"Private DNS using Pi-hole PI-hole is a private DNS server, DNS sinkhole (DNS level ad blocker), DHCP server. Installation You can install Pi-hole on docker: version : \"3\" services : pihole : container_name : pihole image : pihole/pihole:2022.07.1 # For DHCP it is recommended to remove these ports and instead add: network_mode: \"host\" ports : - \"53:53/tcp\" - \"53:53/udp\" - \"67:67/udp\" - \"8080:80/tcp\" environment : TZ : 'Europe/Kiev' DNSMASQ_USER : root WEBPASSWORD : somepass volumes : - '/home/linaro/docker/pihole/etc-pihole:/etc/pihole' - '/home/linaro/docker/pihole/etc-dnsmasq.d:/etc/dnsmasq.d' cap_add : - NET_ADMIN restart : always Docker Pi-hole Github page Add DNS entries First, you need to add a DNS record. Press Local DNS -> DNS Records . Add a desired domain and associated IP address. Then, you can add as many subdomains as you want using CNAME records menu. The subdomains path should point to the main domain name, which was created previously: Using Pi-hole DNS You can add Pi-hole instance IP address as a DNS server for individual machine (using network settings), or you can add it to tour router settings, so all the machines in your network can use it.","title":"Private DNS using Pi-hole"},{"location":"guides/pihole-dns/#private-dns-using-pi-hole","text":"PI-hole is a private DNS server, DNS sinkhole (DNS level ad blocker), DHCP server.","title":"Private DNS using Pi-hole"},{"location":"guides/pihole-dns/#installation","text":"You can install Pi-hole on docker: version : \"3\" services : pihole : container_name : pihole image : pihole/pihole:2022.07.1 # For DHCP it is recommended to remove these ports and instead add: network_mode: \"host\" ports : - \"53:53/tcp\" - \"53:53/udp\" - \"67:67/udp\" - \"8080:80/tcp\" environment : TZ : 'Europe/Kiev' DNSMASQ_USER : root WEBPASSWORD : somepass volumes : - '/home/linaro/docker/pihole/etc-pihole:/etc/pihole' - '/home/linaro/docker/pihole/etc-dnsmasq.d:/etc/dnsmasq.d' cap_add : - NET_ADMIN restart : always Docker Pi-hole Github page","title":"Installation"},{"location":"guides/pihole-dns/#add-dns-entries","text":"First, you need to add a DNS record. Press Local DNS -> DNS Records . Add a desired domain and associated IP address. Then, you can add as many subdomains as you want using CNAME records menu. The subdomains path should point to the main domain name, which was created previously:","title":"Add DNS entries"},{"location":"guides/pihole-dns/#using-pi-hole-dns","text":"You can add Pi-hole instance IP address as a DNS server for individual machine (using network settings), or you can add it to tour router settings, so all the machines in your network can use it.","title":"Using Pi-hole DNS"},{"location":"guides/portainer/","text":"Deploy to Docker using Portainer Portainer is a Docker management tool, which can be deployed to Docker itself. It can be used as a standalone service, or as a server-agent combination. With the last one you can install portainer agents to multiple machines, and manage them from portainer server. Portainer Agent Installed on all Docker machines: version : \"3.6\" services : portainer-agent : image : portainer/agent:latest container_name : portainer-agent restart : always volumes : - /etc/localtime:/etc/localtime:ro - /var/run/docker.sock:/var/run/docker.sock:ro ports : - 9001:9001 Portainer Server Installed on a single machine (control plane): version : \"3.6\" services : portainer-server : image : portainer/portainer-ce:latest container_name : portainer-server restart : always volumes : - ./portainer-data:/data ports : - 8000:8000 - 9443:9443 environment : - SERVER_IP=${SERVER_IP} In Portainer server UI you can add machines you want to manage: Docker Compose stack deploy In Portainer UI select Stacks -> Add stack Add your docker-compose yaml configuration: Press deploy stack. That's it, the services were deployed to Docker. You can add DNS record with readable host name using Pi-hole, and bind the traffic to specific ports using Nginx Proxy Manager.","title":"Deploy to Docker using Portainer"},{"location":"guides/portainer/#deploy-to-docker-using-portainer","text":"Portainer is a Docker management tool, which can be deployed to Docker itself. It can be used as a standalone service, or as a server-agent combination. With the last one you can install portainer agents to multiple machines, and manage them from portainer server.","title":"Deploy to Docker using Portainer"},{"location":"guides/portainer/#portainer-agent","text":"Installed on all Docker machines: version : \"3.6\" services : portainer-agent : image : portainer/agent:latest container_name : portainer-agent restart : always volumes : - /etc/localtime:/etc/localtime:ro - /var/run/docker.sock:/var/run/docker.sock:ro ports : - 9001:9001","title":"Portainer Agent"},{"location":"guides/portainer/#portainer-server","text":"Installed on a single machine (control plane): version : \"3.6\" services : portainer-server : image : portainer/portainer-ce:latest container_name : portainer-server restart : always volumes : - ./portainer-data:/data ports : - 8000:8000 - 9443:9443 environment : - SERVER_IP=${SERVER_IP} In Portainer server UI you can add machines you want to manage:","title":"Portainer Server"},{"location":"guides/portainer/#docker-compose-stack-deploy","text":"In Portainer UI select Stacks -> Add stack Add your docker-compose yaml configuration: Press deploy stack. That's it, the services were deployed to Docker. You can add DNS record with readable host name using Pi-hole, and bind the traffic to specific ports using Nginx Proxy Manager.","title":"Docker Compose stack deploy"},{"location":"guides/pxe/","text":"OS installation over the network How to install an OS using using PXE boot PXE boot The PXE environment is the process of having your device boot from its network card. To boot the device into the PXE environment, the device should get an instructions such as where to take a config and OS image. The most common way of trying to do this is to configure DHCP server to store and serve this information. We send a Wake on Lan package to turn on the device. Device is configured to boot from PXE (via BIOS) The device ask a DHCP server (configured to know PXE server details) for an IP and the TFTP server address The device fetch the bootloader (boot configuration, kernel, initial ramdisk) and the HTTP server address. Then it fetch the installation configuration (such as initial user to create, network configuration, system language, etc) and an OS iso image to install. After that, the OS installation started in automatic mode. PXE implementation Ansible + Docker are used to create a working PXE environment. All the required servers is running on docker. Ansible is used to make an initial configuration (network config, download and mount ISO image, etc) of the PXE environment and trigger the device to power on.","title":"OS installation over the network"},{"location":"guides/pxe/#os-installation-over-the-network","text":"How to install an OS using using PXE boot","title":"OS installation over the network"},{"location":"guides/pxe/#pxe-boot","text":"The PXE environment is the process of having your device boot from its network card. To boot the device into the PXE environment, the device should get an instructions such as where to take a config and OS image. The most common way of trying to do this is to configure DHCP server to store and serve this information. We send a Wake on Lan package to turn on the device. Device is configured to boot from PXE (via BIOS) The device ask a DHCP server (configured to know PXE server details) for an IP and the TFTP server address The device fetch the bootloader (boot configuration, kernel, initial ramdisk) and the HTTP server address. Then it fetch the installation configuration (such as initial user to create, network configuration, system language, etc) and an OS iso image to install. After that, the OS installation started in automatic mode.","title":"PXE boot"},{"location":"guides/pxe/#pxe-implementation","text":"Ansible + Docker are used to create a working PXE environment. All the required servers is running on docker. Ansible is used to make an initial configuration (network config, download and mount ISO image, etc) of the PXE environment and trigger the device to power on.","title":"PXE implementation"},{"location":"installation/hardware/","text":"Hardware configuration To prepare the machines for the automatic OS provisioning you should make some initial setup to enable required features. BIOS setup Common settings Enable Wake-on-LAN (WoL) Enable network boot (PXE Boot) Use UEFI mode and disable CSM (legacy) mode Disable secure boot Boot order options SSD/HDD PXE USB If no bootable operation system found on the main disk, then the computer should try to boot via network. After OS installation it will boot into it. Here is an example BIOS config (may be different on your machine): Devices : NetworkSetup : PXEIPv4 : true PXEIPv6 : false Advanced : CPUSetup : VT-d : true Power : AutomaticPowerOn : WoL : Automatic Security : SecureBoot : false Startup : CSM : false Network setup Connect the devices to the network (wired LAN connection). It should be the same network with initial controller machine. Bind a static IP address for each machine by it's MAC address (for example though the router web UI)","title":"Hardware configuration"},{"location":"installation/hardware/#hardware-configuration","text":"To prepare the machines for the automatic OS provisioning you should make some initial setup to enable required features.","title":"Hardware configuration"},{"location":"installation/hardware/#bios-setup","text":"","title":"BIOS setup"},{"location":"installation/hardware/#common-settings","text":"Enable Wake-on-LAN (WoL) Enable network boot (PXE Boot) Use UEFI mode and disable CSM (legacy) mode Disable secure boot","title":"Common settings"},{"location":"installation/hardware/#boot-order-options","text":"SSD/HDD PXE USB If no bootable operation system found on the main disk, then the computer should try to boot via network. After OS installation it will boot into it. Here is an example BIOS config (may be different on your machine): Devices : NetworkSetup : PXEIPv4 : true PXEIPv6 : false Advanced : CPUSetup : VT-d : true Power : AutomaticPowerOn : WoL : Automatic Security : SecureBoot : false Startup : CSM : false","title":"Boot order options"},{"location":"installation/hardware/#network-setup","text":"Connect the devices to the network (wired LAN connection). It should be the same network with initial controller machine. Bind a static IP address for each machine by it's MAC address (for example though the router web UI)","title":"Network setup"},{"location":"installation/infrastructure/","text":"Infrastructure provisioning The base of the infrastructure of the dev-lab is Docker and Kubernetes (k3s). The installation of docker and k3s is done via Ansible. Docker Docker is an open platform to create, deploy and manage virtualized application containers on a common operating system. docker-compose is a tool for defining and running multi-container Docker applications. With Compose, you use a YAML file to configure your application's services. Installation : Ansible playbook k3s K3s is a highly available, certified Kubernetes distribution designed for production workloads in unattended, resource-constrained, remote locations or inside IoT appliances. K3s is packaged as a single <50MB binary that reduces the dependencies and steps needed to install, run and auto-update a production Kubernetes cluster. K3s works great from something as small as a Raspberry Pi to an AWS a1.4xlarge 32GiB server. Project Page Installation : Ansible playbook When the setup steps for all nodes is complete, you\u2019re ready to access the cluster remotely: SSH into the master node and copy the contents of /etc/rancher/k3s/k3s.yaml to ~/.kube/config on your local machine. Replace the server: key\u2019s IP address(127.0.0.1) with your master node\u2019s public IP address. You can leave the port 6443 as is. Now you can access the cluster using kubectl or Lens","title":"Infrastructure provisioning"},{"location":"installation/infrastructure/#infrastructure-provisioning","text":"The base of the infrastructure of the dev-lab is Docker and Kubernetes (k3s). The installation of docker and k3s is done via Ansible.","title":"Infrastructure provisioning"},{"location":"installation/infrastructure/#docker","text":"Docker is an open platform to create, deploy and manage virtualized application containers on a common operating system. docker-compose is a tool for defining and running multi-container Docker applications. With Compose, you use a YAML file to configure your application's services. Installation : Ansible playbook","title":"Docker"},{"location":"installation/infrastructure/#k3s","text":"K3s is a highly available, certified Kubernetes distribution designed for production workloads in unattended, resource-constrained, remote locations or inside IoT appliances. K3s is packaged as a single <50MB binary that reduces the dependencies and steps needed to install, run and auto-update a production Kubernetes cluster. K3s works great from something as small as a Raspberry Pi to an AWS a1.4xlarge 32GiB server. Project Page Installation : Ansible playbook When the setup steps for all nodes is complete, you\u2019re ready to access the cluster remotely: SSH into the master node and copy the contents of /etc/rancher/k3s/k3s.yaml to ~/.kube/config on your local machine. Replace the server: key\u2019s IP address(127.0.0.1) with your master node\u2019s public IP address. You can leave the port 6443 as is. Now you can access the cluster using kubectl or Lens","title":"k3s"},{"location":"installation/os/","text":"OS setup Installing OS on bare metal over the network The OS installation playbook has been tested with Ubuntu 20.04. If you want to install Debian or Fedora, you should change cloud-init files to match the OS requirements. The easy way to get cloud-init config files is to copy it from the working machine. For the Ubuntu 20.04 distribution it is located by /var/log/installer/autoinstall-user-data Prerequisites Initial controller The initial controller is the machine used to bootstrap the servers, we only need it once, you can use your laptop or desktop A Linux machine that can run Docker (because the host networking driver used for PXE boot only supports Linux, you can use a Linux virtual machine with bridged networking if you're on macOS or Windows). The following ports should be free and not blocked by firewall: service port protocol DHCP 67 UDP TFTP 69 UDP HTTP 80 TCP DNS 53 TCP/UDP Tools Ansible . Also some basic collections required: community.general and community.docker Docker and docker-compose Server machines Any modern x86_64 computer(s) should work, you can use old PCs, laptops or servers, but they should support: - PXE boot - ability to boot from the network - Wake-on-LAN capability - used to wake the machines up automatically without physically touching the power button Variables Vars should be defined in vars/main.yaml file: variable description notes os_image_url Direct url of OS live-server distribution Ubuntu daily live iso is here os_username Username of the user to be created The same user will be created on all machines os_password User's password Should be encrypted Hosts information should be defined in hosts file. Before install you should collect the following information about your server machines: - MAC address - Disk name (for example /dev/sda) - Network interface name (for example eno1) - Choose a desired static IP address for each machine Deployment OS installation is going to be performed via network using PXE boot. PXE is a client-server interface that allows computers in a network to be booted from the server before deploying the obtained OS image. The OS deployment workflow via PXE will look like this: We will use dnsmasq for DHCP + TFTP servers and nginx for HTTP server. The dnsmasq docker image based on @axtstar 's Dockerfile","title":"OS setup"},{"location":"installation/os/#os-setup","text":"Installing OS on bare metal over the network The OS installation playbook has been tested with Ubuntu 20.04. If you want to install Debian or Fedora, you should change cloud-init files to match the OS requirements. The easy way to get cloud-init config files is to copy it from the working machine. For the Ubuntu 20.04 distribution it is located by /var/log/installer/autoinstall-user-data","title":"OS setup"},{"location":"installation/os/#prerequisites","text":"","title":"Prerequisites"},{"location":"installation/os/#initial-controller","text":"The initial controller is the machine used to bootstrap the servers, we only need it once, you can use your laptop or desktop A Linux machine that can run Docker (because the host networking driver used for PXE boot only supports Linux, you can use a Linux virtual machine with bridged networking if you're on macOS or Windows). The following ports should be free and not blocked by firewall: service port protocol DHCP 67 UDP TFTP 69 UDP HTTP 80 TCP DNS 53 TCP/UDP","title":"Initial controller"},{"location":"installation/os/#tools","text":"Ansible . Also some basic collections required: community.general and community.docker Docker and docker-compose","title":"Tools"},{"location":"installation/os/#server-machines","text":"Any modern x86_64 computer(s) should work, you can use old PCs, laptops or servers, but they should support: - PXE boot - ability to boot from the network - Wake-on-LAN capability - used to wake the machines up automatically without physically touching the power button","title":"Server machines"},{"location":"installation/os/#variables","text":"Vars should be defined in vars/main.yaml file: variable description notes os_image_url Direct url of OS live-server distribution Ubuntu daily live iso is here os_username Username of the user to be created The same user will be created on all machines os_password User's password Should be encrypted Hosts information should be defined in hosts file. Before install you should collect the following information about your server machines: - MAC address - Disk name (for example /dev/sda) - Network interface name (for example eno1) - Choose a desired static IP address for each machine","title":"Variables"},{"location":"installation/os/#deployment","text":"OS installation is going to be performed via network using PXE boot. PXE is a client-server interface that allows computers in a network to be booted from the server before deploying the obtained OS image. The OS deployment workflow via PXE will look like this: We will use dnsmasq for DHCP + TFTP servers and nginx for HTTP server. The dnsmasq docker image based on @axtstar 's Dockerfile","title":"Deployment"},{"location":"installation/services/","text":"Service Layer For now the service layer includes such tools as: Gitea : a s self-hosted Git service. It is similar to GitHub, Bitbucket, and GitLab. Drone : a self-service Continuous Integration platform Longhorn : a highly available persistent storage for Kubernetes Portainer : a self-service Docker containers delivery platform All these services is going to be installed on k3s cluster automatically using helm. For manual installation instructions see Guides section.","title":"Service Layer"},{"location":"installation/services/#service-layer","text":"For now the service layer includes such tools as: Gitea : a s self-hosted Git service. It is similar to GitHub, Bitbucket, and GitLab. Drone : a self-service Continuous Integration platform Longhorn : a highly available persistent storage for Kubernetes Portainer : a self-service Docker containers delivery platform All these services is going to be installed on k3s cluster automatically using helm. For manual installation instructions see Guides section.","title":"Service Layer"}]}